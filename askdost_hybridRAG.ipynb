{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62d58e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import streamlit as st\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_qdrant import Qdrant\n",
    "from tavily import TavilyClient\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43bfac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load API keys from .env ---\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95d35f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM Setup ---\n",
    "primary_qa_llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88c42a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. CACHE HEAVY OBJECTS\n",
    "# -----------------------------\n",
    "@st.cache_resource\n",
    "def load_llm():\n",
    "    return ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "@st.cache_resource\n",
    "def load_vectorstore():\n",
    "    loader = PyMuPDFLoader(\"data/2025-DOST-SEI-ST-Scholars-Handbook-Final.pdf\")\n",
    "    documents = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=600,\n",
    "        chunk_overlap=150,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "    return Qdrant.from_documents(\n",
    "        chunks,\n",
    "        embeddings,\n",
    "        location=\":memory:\",\n",
    "        collection_name=\"DOST_Handbook\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a18a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 12:07:19.718 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-26 12:07:19.940 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\Mark Gelson\\Desktop\\Predictive Systems AI\\Demo Day\\day_4\\.venv\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-09-26 12:07:19.941 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-26 12:07:19.941 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-26 12:07:19.941 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-26 12:07:19.945 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-26 12:07:19.946 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-26 12:07:19.947 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-26 12:07:19.948 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-26 12:07:19.949 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-26 12:07:19.949 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-26 12:07:19.950 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-26 12:07:20.464 Thread 'Thread-4': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-26 12:07:20.466 Thread 'Thread-4': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-26 12:07:20.467 Thread 'Thread-4': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-26 12:07:26.716 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-26 12:07:26.717 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-26 12:07:26.718 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# --- Init core retrievers ---\n",
    "llm = load_llm()\n",
    "qdrant_vector_store = load_vectorstore()\n",
    "\n",
    "retriever = qdrant_vector_store.as_retriever(search_kwargs={\"k\": 7})\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever,\n",
    ")\n",
    "advanced_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=compression_retriever,\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef77c87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mark Gelson\\AppData\\Local\\Temp\\ipykernel_38200\\1455303568.py:22: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  handbook_chain = LLMChain(llm=primary_qa_llm, prompt=handbook_prompt)\n"
     ]
    }
   ],
   "source": [
    "# --- Custom Prompt (encourages detailed answers) ---\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "handbook_prompt = PromptTemplate.from_template( \"\"\"\n",
    "You are an assistant for DOST-SEI scholars ans wering questions using the official handbook.\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "1. Answer ONLY based on the provided context below\n",
    "2. Do NOT add information not present in the context\n",
    "3. If you're unsure, say \"I need more specific information from the handbook\" or \"Can you give mo more context to your question?\"\n",
    "4. Quote specific sections when possible\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{input}\n",
    "\n",
    "Answer clearly and concisely.\n",
    "\"\"\" )\n",
    "\n",
    "handbook_chain = LLMChain(llm=primary_qa_llm, prompt=handbook_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f9570d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Document Chain and Retrieval Chain ---\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import LLMChain, create_retrieval_chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(primary_qa_llm, handbook_prompt)\n",
    "retrieval_chain = create_retrieval_chain(advanced_retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6122056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Guardrails ---\n",
    "def guardrails_pre_query(user_input: str) -> str:\n",
    "    blocked = [\"violence\", \"politics\", \"religion\"]\n",
    "    if any(b in user_input.lower() for b in blocked):\n",
    "        return \"Sorry, I cannot answer that type of question.\"\n",
    "    return user_input\n",
    "\n",
    "def guardrails_post_response(response: str) -> str:\n",
    "    if \"I don't know\" in response or len(response.strip()) < 20:\n",
    "        response += \"\\n\\n⚠️ This answer may be incomplete. Please verify in the official handbook.\"\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b42bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Judge Chain ---\n",
    "fallback_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a judge\n",
    "Given the user question and the handbook answer,\n",
    "decide if the answer is sufficient, accurate, and complete.\n",
    "\n",
    "Question: {question}\n",
    "Answer: {answer}\n",
    "\n",
    "Respond with ONLY one word: \"sufficient\" or \"insufficient\".\n",
    "\"\"\")\n",
    "judge_chain = LLMChain(llm=primary_qa_llm, prompt=fallback_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "357771f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from tavily import TavilyClient\n",
    "\n",
    "tavily = TavilyClient(api_key=TAVILY_API_KEY)\n",
    "\n",
    "def tavily_search(query: str, max_results: int = 5) -> tuple[str, list]:\n",
    "    # Step 1: Try trusted domains\n",
    "    preferred_domains = [\n",
    "        \"sei.dost.gov.ph\",\n",
    "        \"dost.gov.ph\",\n",
    "        \"facebook.com/DOST.SEI\",\n",
    "        \"facebook.com/DOSTph\",\n",
    "        \"science-scholarships.ph\",\n",
    "    ]\n",
    "\n",
    "    results = tavily.search(\n",
    "        query,\n",
    "        max_results=max_results,\n",
    "        include_domains=preferred_domains\n",
    "    )\n",
    "\n",
    "    # Step 2: Fall back to open web if nothing\n",
    "    if not results[\"results\"]:\n",
    "         results = tavily.search(query, max_results=max_results)\n",
    "\n",
    "    # Step 3: Format with dates\n",
    "    parsed = []\n",
    "    summaries = []\n",
    "    for r in results[\"results\"]:\n",
    "        pub_date = r.get(\"published_date\")\n",
    "        if pub_date:\n",
    "            try:\n",
    "                pub_date = datetime.fromisoformat(pub_date.replace(\"Z\", \"+00:00\"))\n",
    "                date_str = pub_date.strftime(\"%Y-%m-%d\")\n",
    "            except Exception:\n",
    "                date_str = \"N/A\"\n",
    "        else:\n",
    "            date_str = \"N/A\"\n",
    "\n",
    "        summaries.append(f\"[{date_str}] {r['title']}: {r['content']}\")\n",
    "        parsed.append({\"title\": r[\"title\"], \"url\": r[\"url\"], \"date\": date_str})\n",
    "\n",
    "    return \"\\n\".join(summaries), parsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f3e8d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stronger web prompt that requires recency & citations ---\n",
    "web_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an assistant for DOST-SEI scholars. Use ONLY the top search results below to answer the question.\n",
    "IMPORTANT:\n",
    "- PRIORITIZE the most recent information.\n",
    "- If the results do not contain recent/definitive information, reply exactly: \"I couldn't find recent information on this topic.\"\n",
    "\n",
    "Top search results (most recent first):\n",
    "{results}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer (use only the search results above):\n",
    "\"\"\")\n",
    "\n",
    "web_chain = LLMChain(llm=primary_qa_llm, prompt=web_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7733acf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 12:17:09.804 No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "@st.cache_data(show_spinner=\"Answering ...\")\n",
    "def hybrid_agent(question: str, history: list = None) -> str:\n",
    "    \"\"\"\n",
    "    Hybrid agent with handbook RAG + web fallback.\n",
    "    Accepts optional conversation history for follow-ups.\n",
    "    Uses Streamlit caching to speed up repeated queries.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Step 0: Use conversation memory ---\n",
    "    context_from_history = \"\"\n",
    "    if history:\n",
    "        # Take only last 3 exchanges to keep context focused\n",
    "        context_from_history = \"\\n\\nRecent conversation:\\n\" + \"\\n\".join(\n",
    "            [f\"{m['role']}: {m['content']}\" for m in history[-3:]]\n",
    "        )\n",
    "\n",
    "    # Append history to user question\n",
    "    full_query = question + context_from_history\n",
    "\n",
    "    # --- Step 1: Guardrails (pre-query) ---\n",
    "    safe_query = guardrails_pre_query(full_query)\n",
    "    if safe_query.startswith(\"Sorry\"):\n",
    "        return safe_query\n",
    "\n",
    "    # --- Step 2: Handbook RAG ---\n",
    "    handbook_response = retrieval_chain.invoke({\"input\": safe_query})\n",
    "    answer = handbook_response[\"answer\"]\n",
    "    answer = guardrails_post_response(answer)\n",
    "\n",
    "    # --- Step 3: Judge sufficiency ---\n",
    "    judge_result = judge_chain.invoke({\"question\": safe_query, \"answer\": answer})\n",
    "    if \"insufficient\" in judge_result[\"text\"].strip().lower():\n",
    "        results_text, parsed_results = tavily_search(safe_query)\n",
    "        web_answer = web_chain.invoke({\n",
    "            \"results\": results_text,\n",
    "            \"question\": safe_query\n",
    "        })\n",
    "        return web_answer[\"text\"]\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac51fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
